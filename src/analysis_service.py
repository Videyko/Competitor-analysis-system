#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ú—ñ–∫—Ä–æ—Å–µ—Ä–≤—ñ—Å –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –ø–∞—Ä—Ç–Ω–µ—Ä—Å—å–∫–∏—Ö/–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤
REST API –Ω–∞ –±–∞–∑—ñ FastAPI
"""

import os
import sys
sys.path.append('/app')

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from pydantic import BaseModel, HttpUrl, Field
from typing import List, Dict, Optional
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from openai import OpenAI
import re
from urllib.parse import urljoin, urlparse
import json
import time
from dataclasses import dataclass, asdict
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from collections import defaultdict
import uuid
from datetime import datetime

# –õ–æ–∫–∞–ª—å–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏
from shared.logger import setup_logger
from shared.database import db_manager
from shared.redis_client import analysis_redis, analysis_cache
from shared.utils import generate_task_id, clean_url, ProgressTracker

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logger = setup_logger('analysis_service')

# Pydantic –º–æ–¥–µ–ª—ñ –¥–ª—è API
class AnalysisRequest(BaseModel):
    site_url: HttpUrl = Field(..., description="URL —Å–∞–π—Ç—É –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
    positive_keywords: List[str] = Field(..., description="–ü–æ–∑–∏—Ç–∏–≤–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞")
    negative_keywords: List[str] = Field(..., description="–ù–µ–≥–∞—Ç–∏–≤–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞")
    max_time_minutes: int = Field(default=20, ge=1, le=60, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —á–∞—Å –∞–Ω–∞–ª—ñ–∑—É (—Ö–≤)")
    max_links: int = Field(default=300, ge=10, le=1000, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Å–∏–ª–∞–Ω—å")
    openai_api_key: Optional[str] = Field(default=None, description="OpenAI API –∫–ª—é—á")

class AnalysisStatus(BaseModel):
    task_id: str
    status: str  # "pending", "running", "completed", "failed"
    progress: int  # 0-100
    message: str
    started_at: datetime
    completed_at: Optional[datetime] = None

class KeywordMatch(BaseModel):
    keyword: str
    url: str
    count: int
    context: str

class AnalysisResult(BaseModel):
    task_id: str
    site_url: str
    status: str
    pages_analyzed: int
    positive_matches: List[KeywordMatch]
    negative_matches: List[KeywordMatch]
    ai_analysis: Optional[str]
    analysis_time: float
    pages_with_positive: List[str]
    pages_with_negative: List[str]
    summary_stats: Dict[str, int]
    detailed_stats: Dict
    completed_at: datetime

@dataclass
class SimpleAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É —Å–∞–π—Ç—É"""
    site_url: str
    pages_analyzed: int
    keyword_table: pd.DataFrame
    forbidden_table: pd.DataFrame
    ai_analysis: str
    analysis_time: float
    pages_with_keywords: List[str]
    pages_with_forbidden: List[str]
    detailed_stats: Dict

class AsyncPartnerSiteAnalyzer:
    def __init__(self, openai_api_key: str = None, max_workers: int = 5):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞
        """
        self.openai_api_key = openai_api_key
        if openai_api_key:
            self.client = OpenAI(api_key=openai_api_key)
        else:
            self.client = None
            
        self.max_workers = max_workers
        self.lock = threading.Lock()
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å–µ—Å—ñ—ó
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
    async def find_all_links(self, base_url: str, max_links: int = 300) -> set:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å–∞–π—Ç—ñ
        """
        logger.info(f"üîç –®—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ {base_url}")
        
        domain = urlparse(base_url).netloc.replace('www.', '')
        found_links = set()
        links_to_check = {base_url}
        checked_links = set()
        
        # –î–æ–¥–∞—î–º–æ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –≥–æ–ª–æ–≤–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        variations = [
            f"https://{domain}",
            f"https://www.{domain}",
            f"http://{domain}",
            f"http://www.{domain}"
        ]
        links_to_check.update(variations)
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            while links_to_check and len(found_links) < max_links:
                current_url = links_to_check.pop()
                
                if current_url in checked_links:
                    continue
                    
                checked_links.add(current_url)
                
                try:
                    logger.info(f"üìÑ –°–∫–∞–Ω—É—é: {current_url}")
                    async with session.get(current_url, timeout=10) as response:
                        if response.status == 200:
                            content = await response.text()
                            soup = BeautifulSoup(content, 'html.parser')
                            
                            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
                            for link in soup.find_all('a', href=True):
                                href = link['href']
                                full_url = urljoin(current_url, href)
                                parsed_url = urlparse(full_url)
                                
                                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑ —Ç–æ–≥–æ –∂ –¥–æ–º–µ–Ω—É
                                if (parsed_url.netloc.endswith(domain) or 
                                    parsed_url.netloc == domain or
                                    parsed_url.netloc == f"www.{domain}"):
                                    
                                    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω–µ–±–∞–∂–∞–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
                                    if not any(skip in full_url.lower() for skip in 
                                             ['#', 'javascript:', 'mailto:', 'tel:', '.pdf', '.jpg', 
                                              '.png', '.gif', '.zip', '.rar', '.exe', '.doc', '.docx', 
                                              '.xls', '.xlsx', 'wp-admin', 'wp-content']):
                                        
                                        # –û—á–∏—â—É—î–º–æ URL
                                        clean_url_result = clean_url(full_url)
                                        
                                        if clean_url_result and clean_url_result not in found_links:
                                            found_links.add(clean_url_result)
                                            
                                            # –î–æ–¥–∞—î–º–æ –¥–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –≤–∞–∂–ª–∏–≤—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
                                            if any(keyword in clean_url_result.lower() for keyword in 
                                                  ['product', 'catalog', 'category', '—Ç–æ–≤–∞—Ä', '–∫–∞—Ç–∞–ª–æ–≥', 
                                                   '–∫–∞—Ç–µ–≥–æ—Ä', '–Ω–æ–≤–∏–Ω', 'news', 'about', 'contact']):
                                                if clean_url_result not in checked_links:
                                                    links_to_check.add(clean_url_result)
                    
                    await asyncio.sleep(0.5)  # –ü–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
                    
                except Exception as e:
                    logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è {current_url}: {e}")
                    continue
        
        logger.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(found_links)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –ø–æ—Å–∏–ª–∞–Ω—å")
        return found_links
    
    async def scrape_page_content(self, session: aiohttp.ClientSession, url: str) -> tuple:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–∫—Ä–∞–ø–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –æ–¥–Ω—ñ—î—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        """
        try:
            async with session.get(url, timeout=15) as response:
                if response.status == 200:
                    content = await response.text()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # –í–∏–¥–∞–ª—è—î–º–æ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏
                    for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                        element.decompose()
                    
                    # –í–∏—Ç—è–≥—É—î–º–æ —Ç–µ–∫—Å—Ç
                    text_content = soup.get_text(separator=' ', strip=True)
                    text_content = re.sub(r'\s+', ' ', text_content)
                    
                    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
                    if len(text_content) < 100:
                        return None, None
                    
                    logger.info(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ: {url} ({len(text_content)} —Å–∏–º–≤–æ–ª—ñ–≤)")
                    return url, text_content
                    
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ {url}: {e}")
            
        return None, None
    
    async def scrape_all_pages(self, links: set, max_time_minutes: int = 20) -> dict:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–∫—Ä–∞–ø–∏—Ç—å –≤—Å—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        """
        start_time = time.time()
        max_time_seconds = max_time_minutes * 60
        
        logger.info(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∫–æ–Ω—Ç–µ–Ω—Ç –∑ {len(links)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫...")
        
        pages_content = {}
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            tasks = []
            for url in links:
                if time.time() - start_time > max_time_seconds:
                    break
                task = asyncio.create_task(self.scrape_page_content(session, url))
                tasks.append(task)
            
            completed = 0
            for task in asyncio.as_completed(tasks):
                if time.time() - start_time > max_time_seconds:
                    logger.info("‚è∞ –î–æ—Å—è–≥–Ω—É—Ç–æ –ª—ñ–º—ñ—Ç —á–∞—Å—É!")
                    break
                
                url, content = await task
                if url and content:
                    pages_content[url] = content
                
                completed += 1
                if completed % 20 == 0:
                    logger.info(f"üìä –û–±—Ä–æ–±–ª–µ–Ω–æ {completed}/{len(links)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫")
        
        logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç –∑ {len(pages_content)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫")
        return pages_content
    
    def search_keywords_static(self, pages_content: dict, 
                              keywords: List[str], forbidden_words: List[str]) -> tuple:
        """
        –°—Ç–∞—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –∑ –¥–µ—Ç–∞–ª—å–Ω–æ—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é
        """
        logger.info("üîç –°—Ç–∞—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤...")
        
        keyword_data = []
        forbidden_data = []
        
        # –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª–æ–≤–∞—Ö
        keyword_stats = {}
        found_keywords = set()
        
        # –ü–æ—à—É–∫ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
        for keyword in keywords:
            keyword_stats[keyword] = {
                'total_mentions': 0,
                'pages_found': [],
                'contexts': []
            }
            
            for url, content in pages_content.items():
                pattern = re.compile(re.escape(keyword), re.IGNORECASE)
                matches = pattern.findall(content)
                
                if matches:
                    count = len(matches)
                    context = self._extract_context(content, keyword, 200)
                    found_keywords.add(keyword)
                    
                    # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    keyword_stats[keyword]['total_mentions'] += count
                    keyword_stats[keyword]['pages_found'].append({
                        'url': url,
                        'count': count,
                        'context': context
                    })
                    keyword_stats[keyword]['contexts'].append(context)
                    
                    keyword_data.append({
                        '–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ': keyword,
                        'URL': url,
                        '–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–≥–∞–¥–æ–∫': count,
                        '–ö–æ–Ω—Ç–µ–∫—Å—Ç': context
                    })
        
        # –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏–º —Å–ª–æ–≤–∞–º
        forbidden_stats = {}
        found_forbidden = set()
        
        # –ü–æ—à—É–∫ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö —Å–ª—ñ–≤
        for forbidden_word in forbidden_words:
            forbidden_stats[forbidden_word] = {
                'total_mentions': 0,
                'pages_found': [],
                'contexts': []
            }
            
            for url, content in pages_content.items():
                pattern = re.compile(re.escape(forbidden_word), re.IGNORECASE)
                matches = pattern.findall(content)
                
                if matches:
                    count = len(matches)
                    context = self._extract_context(content, forbidden_word, 200)
                    found_forbidden.add(forbidden_word)
                    
                    # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    forbidden_stats[forbidden_word]['total_mentions'] += count
                    forbidden_stats[forbidden_word]['pages_found'].append({
                        'url': url,
                        'count': count,
                        'context': context
                    })
                    forbidden_stats[forbidden_word]['contexts'].append(context)
                    
                    forbidden_data.append({
                        '–ó–∞–±–æ—Ä–æ–Ω–µ–Ω–µ —Å–ª–æ–≤–æ': forbidden_word,
                        'URL': url,
                        '–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–≥–∞–¥–æ–∫': count,
                        '–ö–æ–Ω—Ç–µ–∫—Å—Ç': context
                    })
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –Ω–µ–∑–Ω–∞–π–¥–µ–Ω—ñ —Å–ª–æ–≤–∞
        not_found_keywords = [kw for kw in keywords if kw not in found_keywords]
        not_found_forbidden = [fw for fw in forbidden_words if fw not in found_forbidden]
        
        keyword_df = pd.DataFrame(keyword_data)
        forbidden_df = pd.DataFrame(forbidden_data)
        
        logger.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(keyword_df)} –∑–≥–∞–¥–æ–∫ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤")
        logger.info(f"‚ö†Ô∏è –ó–Ω–∞–π–¥–µ–Ω–æ {len(forbidden_df)} –∑–≥–∞–¥–æ–∫ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö —Å–ª—ñ–≤")
        logger.info(f"üìä –ö–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –∑–Ω–∞–π–¥–µ–Ω–æ: {len(found_keywords)}/{len(keywords)}")
        logger.info(f"üìä –ó–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö —Å–ª—ñ–≤ –∑–Ω–∞–π–¥–µ–Ω–æ: {len(found_forbidden)}/{len(forbidden_words)}")
        
        if not_found_keywords:
            logger.info(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤: {', '.join(not_found_keywords)}")
        if not_found_forbidden:
            logger.info(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö —Å–ª—ñ–≤: {', '.join(not_found_forbidden)}")
        
        # –î–æ–¥–∞—î–º–æ –¥–µ—Ç–∞–ª—å–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        detailed_stats = {
            'keyword_stats': keyword_stats,
            'forbidden_stats': forbidden_stats,
            'not_found_keywords': not_found_keywords,
            'not_found_forbidden': not_found_forbidden,
            'summary': {
                'total_keywords': len(keywords),
                'found_keywords': len(found_keywords),
                'not_found_keywords': len(not_found_keywords),
                'total_forbidden': len(forbidden_words),
                'found_forbidden': len(found_forbidden),
                'not_found_forbidden': len(not_found_forbidden)
            }
        }
        
        return keyword_df, forbidden_df, detailed_stats
    
    def _extract_context(self, text: str, keyword: str, context_length: int = 200) -> str:
        """–í–∏—Ç—è–≥—É—î –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–≤–∫–æ–ª–æ –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        match = pattern.search(text)
        
        if match:
            start = max(0, match.start() - context_length)
            end = min(len(text), match.end() + context_length)
            context = text[start:end].strip()
            
            # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤
            words = context.split()
            if len(words) > 40:
                words = words[:40]
                context = ' '.join(words) + "..."
            
            return f"...{context}..."
        
        return ""
    
    def ai_analyze_relevant_pages(self, pages_content: dict, 
                                 keyword_df: pd.DataFrame, forbidden_df: pd.DataFrame) -> str:
        """
        –®–Ü –∞–Ω–∞–ª—ñ–∑ —Ç—ñ–ª—å–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫
        """
        if not self.client:
            return "–®–Ü –∞–Ω–∞–ª—ñ–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π (–Ω–µ –≤–∫–∞–∑–∞–Ω–æ API –∫–ª—é—á)"
        
        logger.info("ü§ñ –®–Ü –∞–Ω–∞–ª—ñ–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫...")
        
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
        pages_with_keywords = set()
        if not keyword_df.empty:
            pages_with_keywords = set(keyword_df['URL'].unique())
        
        pages_with_forbidden = set()
        if not forbidden_df.empty:
            pages_with_forbidden = set(forbidden_df['URL'].unique())
        
        relevant_pages = pages_with_keywords.union(pages_with_forbidden)
        
        if not relevant_pages:
            return "–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –∑ –∫–ª—é—á–æ–≤–∏–º–∏ –∞–±–æ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É"
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è –®–Ü
        relevant_content = ""
        for url in list(relevant_pages)[:10]:
            if url in pages_content:
                content_preview = pages_content[url][:1500]
                relevant_content += f"\n--- –°–¢–û–†–Ü–ù–ö–ê: {url} ---\n{content_preview}\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        keyword_stats = {}
        if not keyword_df.empty:
            keyword_stats = keyword_df.groupby('–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ')['–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–≥–∞–¥–æ–∫'].sum().to_dict()
        
        forbidden_stats = {}
        if not forbidden_df.empty:
            forbidden_stats = forbidden_df.groupby('–ó–∞–±–æ—Ä–æ–Ω–µ–Ω–µ —Å–ª–æ–≤–æ')['–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–≥–∞–¥–æ–∫'].sum().to_dict()
        
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –ø—Ä–∏—Å—É—Ç–Ω—ñ—Å—Ç—å –±—Ä–µ–Ω–¥—É –Ω–∞ –ø–∞—Ä—Ç–Ω–µ—Ä—Å—å–∫–æ–º—É —Å–∞–π—Ç—ñ:

–°–¢–ê–¢–ò–°–¢–ò–ö–ê:
- –í—Å—å–æ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ —Å—Ç–æ—Ä—ñ–Ω–æ–∫: {len(pages_content)}
- –°—Ç–æ—Ä—ñ–Ω–æ–∫ –∑ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏: {len(pages_with_keywords)}
- –°—Ç–æ—Ä—ñ–Ω–æ–∫ –∑ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏–º–∏ —Å–ª–æ–≤–∞–º–∏: {len(pages_with_forbidden)}

–ü–û–ó–ò–¢–ò–í–ù–Ü –ö–õ–Æ–ß–û–í–Ü –°–õ–û–í–ê (–∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–≥–∞–¥–æ–∫):
{json.dumps(keyword_stats, ensure_ascii=False, indent=2)}

–ù–ï–ì–ê–¢–ò–í–ù–Ü –ö–õ–Æ–ß–û–í–Ü –°–õ–û–í–ê (–∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–≥–∞–¥–æ–∫):
{json.dumps(forbidden_stats, ensure_ascii=False, indent=2)}

–ö–û–ù–¢–ï–ù–¢ –†–ï–õ–ï–í–ê–ù–¢–ù–ò–• –°–¢–û–†–Ü–ù–û–ö:
{relevant_content}

–î–∞–π –æ—Ü—ñ–Ω–∫—É —É JSON —Ñ–æ—Ä–º–∞—Ç—ñ:
{{
    "brand_promotion_score": —á–∏—Å–ª–æ_–≤—ñ–¥_0_–¥–æ_100,
    "summary": "–ö–æ—Ä–æ—Ç–∫–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫ –ø—Ä–æ –ø—Ä–∏—Å—É—Ç–Ω—ñ—Å—Ç—å –±—Ä–µ–Ω–¥—É",
    "detailed_analysis": "–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —è–∫–æ—Å—Ç—ñ –ø—Ä–∏—Å—É—Ç–Ω–æ—Å—Ç—ñ",
    "recommendations": "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Å–ø—ñ–≤–ø—Ä–∞—Ü—ñ"
}}
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É –ø—Ä–∏—Å—É—Ç–Ω–æ—Å—Ç—ñ –±—Ä–µ–Ω–¥—ñ–≤ –Ω–∞ –ø–∞—Ä—Ç–Ω–µ—Ä—Å—å–∫–∏—Ö —Å–∞–π—Ç–∞—Ö."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1500
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –®–Ü –∞–Ω–∞–ª—ñ–∑—É: {e}")
            return f"–ü–æ–º–∏–ª–∫–∞ –®–Ü –∞–Ω–∞–ª—ñ–∑—É: {e}"
    
    async def analyze_site(self, site_url: str, keywords: List[str], 
                          forbidden_words: List[str], max_time_minutes: int = 20,
                          max_links: int = 300) -> SimpleAnalysisResult:
        """
        –ü–æ–≤–Ω–∏–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Å–∞–π—Ç—É
        """
        start_time = time.time()
        
        logger.info(f"üöÄ –ü–æ—á–∏–Ω–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑ —Å–∞–π—Ç—É: {site_url}")
        
        # 1. –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
        all_links = await self.find_all_links(site_url, max_links)
        
        # 2. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∫–æ–Ω—Ç–µ–Ω—Ç
        pages_content = await self.scrape_all_pages(all_links, max_time_minutes)
        
        if not pages_content:
            return SimpleAnalysisResult(
                site_url=site_url,
                pages_analyzed=0,
                keyword_table=pd.DataFrame(),
                forbidden_table=pd.DataFrame(),
                ai_analysis="–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç",
                analysis_time=time.time() - start_time,
                pages_with_keywords=[],
                pages_with_forbidden=[],
                detailed_stats={}
            )
        
        # 3. –°—Ç–∞—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
        keyword_df, forbidden_df, detailed_stats = self.search_keywords_static(
            pages_content, keywords, forbidden_words
        )
        
        # 4. –®–Ü –∞–Ω–∞–ª—ñ–∑
        ai_analysis = self.ai_analyze_relevant_pages(pages_content, keyword_df, forbidden_df)
        
        # 5. –ó–±–∏—Ä–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        pages_with_keywords = list(keyword_df['URL'].unique()) if not keyword_df.empty else []
        pages_with_forbidden = list(forbidden_df['URL'].unique()) if not forbidden_df.empty else []
        
        return SimpleAnalysisResult(
            site_url=site_url,
            pages_analyzed=len(pages_content),
            keyword_table=keyword_df,
            forbidden_table=forbidden_df,
            ai_analysis=ai_analysis,
            analysis_time=time.time() - start_time,
            pages_with_keywords=pages_with_keywords,
            pages_with_forbidden=pages_with_forbidden,
            detailed_stats=detailed_stats
        )

# FastAPI –∑–∞—Å—Ç–æ—Å—É–Ω–æ–∫
app = FastAPI(
    title="Competitor Analysis Service",
    description="–ú—ñ–∫—Ä–æ—Å–µ—Ä–≤—ñ—Å –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏—Ö/–ø–∞—Ä—Ç–Ω–µ—Ä—Å—å–∫–∏—Ö —Å–∞–π—Ç—ñ–≤",
    version="1.0.0"
)

# –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
analysis_tasks = {}
analysis_results = {}

@app.get("/")
async def root():
    """–ì–æ–ª–æ–≤–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞"""
    return {
        "message": "Competitor Analysis Service",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.post("/analyze", response_model=Dict[str, str])
async def start_analysis(request: AnalysisRequest, background_tasks: BackgroundTasks):
    """
    –ó–∞–ø—É—Å–∫–∞—î –∞–Ω–∞–ª—ñ–∑ —Å–∞–π—Ç—É –≤ —Ñ–æ–Ω–æ–≤–æ–º—É —Ä–µ–∂–∏–º—ñ
    """
    task_id = generate_task_id()
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á—ñ
    analysis_tasks[task_id] = AnalysisStatus(
        task_id=task_id,
        status="pending",
        progress=0,
        message="–ó–∞–¥–∞—á–∞ —Å—Ç–≤–æ—Ä–µ–Ω–∞",
        started_at=datetime.now()
    )
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑ –≤ —Ñ–æ–Ω–æ–≤–æ–º—É —Ä–µ–∂–∏–º—ñ
    background_tasks.add_task(
        perform_analysis,
        task_id,
        str(request.site_url),
        request.positive_keywords,
        request.negative_keywords,
        request.max_time_minutes,
        request.max_links,
        request.openai_api_key
    )
    
    return {
        "task_id": task_id,
        "status": "pending",
        "message": "–ê–Ω–∞–ª—ñ–∑ –∑–∞–ø—É—â–µ–Ω–æ. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ /status/{task_id} –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å—É"
    }

async def perform_analysis(task_id: str, site_url: str, positive_keywords: List[str], 
                          negative_keywords: List[str], max_time_minutes: int, 
                          max_links: int, openai_api_key: str = None):
    """
    –í–∏–∫–æ–Ω—É—î –∞–Ω–∞–ª—ñ–∑ —Å–∞–π—Ç—É
    """
    try:
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å
        analysis_tasks[task_id].status = "running"
        analysis_tasks[task_id].progress = 10
        analysis_tasks[task_id].message = "–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞..."
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä
        analyzer = AsyncPartnerSiteAnalyzer(openai_api_key)
        
        # –û–Ω–æ–≤–ª—é—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
        analysis_tasks[task_id].progress = 20
        analysis_tasks[task_id].message = "–ü–æ—à—É–∫ –ø–æ—Å–∏–ª–∞–Ω—å..."
        
        # –í–∏–∫–æ–Ω—É—î–º–æ –∞–Ω–∞–ª—ñ–∑
        result = await analyzer.analyze_site(
            site_url=site_url,
            keywords=positive_keywords,
            forbidden_words=negative_keywords,
            max_time_minutes=max_time_minutes,
            max_links=max_links
        )
        
        # –û–Ω–æ–≤–ª—é—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
        analysis_tasks[task_id].progress = 90
        analysis_tasks[task_id].message = "–û–±—Ä–æ–±–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤..."
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É –∑—Ä—É—á–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç
        positive_matches = []
        if not result.keyword_table.empty:
            for _, row in result.keyword_table.iterrows():
                positive_matches.append(KeywordMatch(
                    keyword=row['–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ'],
                    url=row['URL'],
                    count=row['–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–≥–∞–¥–æ–∫'],
                    context=row['–ö–æ–Ω—Ç–µ–∫—Å—Ç']
                ))
        
        negative_matches = []
        if not result.forbidden_table.empty:
            for _, row in result.forbidden_table.iterrows():
                negative_matches.append(KeywordMatch(
                    keyword=row['–ó–∞–±–æ—Ä–æ–Ω–µ–Ω–µ —Å–ª–æ–≤–æ'],
                    url=row['URL'],
                    count=row['–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–≥–∞–¥–æ–∫'],
                    context=row['–ö–æ–Ω—Ç–µ–∫—Å—Ç']
                ))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        summary_stats = {
            "pages_analyzed": result.pages_analyzed,
            "positive_keywords_found": len(positive_matches),
            "negative_keywords_found": len(negative_matches),
            "pages_with_positive": len(result.pages_with_keywords),
            "pages_with_negative": len(result.pages_with_forbidden),
            "analysis_time_seconds": int(result.analysis_time)
        }
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        analysis_results[task_id] = AnalysisResult(
            task_id=task_id,
            site_url=site_url,
            status="completed",
            pages_analyzed=result.pages_analyzed,
            positive_matches=positive_matches,
            negative_matches=negative_matches,
            ai_analysis=result.ai_analysis,
            analysis_time=result.analysis_time,
            pages_with_positive=result.pages_with_keywords,
            pages_with_negative=result.pages_with_forbidden,
            summary_stats=summary_stats,
            detailed_stats=result.detailed_stats,
            completed_at=datetime.now()
        )
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å
        analysis_tasks[task_id].status = "completed"
        analysis_tasks[task_id].progress = 100
        analysis_tasks[task_id].message = "–ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ"
        analysis_tasks[task_id].completed_at = datetime.now()
        
        logger.info(f"‚úÖ –ê–Ω–∞–ª—ñ–∑ {task_id} –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ {task_id}: {e}")
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å –∑ –ø–æ–º–∏–ª–∫–æ—é
        analysis_tasks[task_id].status = "failed"
        analysis_tasks[task_id].progress = 0
        analysis_tasks[task_id].message = f"–ü–æ–º–∏–ª–∫–∞: {str(e)}"
        analysis_tasks[task_id].completed_at = datetime.now()

@app.get("/status/{task_id}", response_model=AnalysisStatus)
async def get_analysis_status(task_id: str):
    """
    –û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç—É—Å –∞–Ω–∞–ª—ñ–∑—É
    """
    if task_id not in analysis_tasks:
        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
    
    return analysis_tasks[task_id]

@app.get("/result/{task_id}", response_model=AnalysisResult)
async def get_analysis_result(task_id: str):
    """
    –û—Ç—Ä–∏–º—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É
    """
    if task_id not in analysis_results:
        if task_id not in analysis_tasks:
            raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
        
        status = analysis_tasks[task_id].status
        if status == "pending" or status == "running":
            raise HTTPException(status_code=202, detail="–ê–Ω–∞–ª—ñ–∑ —â–µ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        elif status == "failed":
            raise HTTPException(status_code=500, detail="–ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–∏–≤—Å—è –∑ –ø–æ–º–∏–ª–∫–æ—é")
    
    return analysis_results[task_id]

@app.get("/tasks")
async def get_all_tasks():
    """
    –û—Ç—Ä–∏–º—É—î —Å–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö –∑–∞–¥–∞—á
    """
    return {
        "tasks": list(analysis_tasks.keys()),
        "total": len(analysis_tasks)
    }

@app.delete("/task/{task_id}")
async def delete_task(task_id: str):
    """
    –í–∏–¥–∞–ª—è—î –∑–∞–¥–∞—á—É —Ç–∞ —ó—ó —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    """
    deleted = False
    
    if task_id in analysis_tasks:
        del analysis_tasks[task_id]
        deleted = True
    
    if task_id in analysis_results:
        del analysis_results[task_id]
        deleted = True
    
    if not deleted:
        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
    
    return {"message": "–ó–∞–¥–∞—á–∞ –≤–∏–¥–∞–ª–µ–Ω–∞"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)